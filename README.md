# Deep Learning Solution for Precise Subtitle Segmentation
Anush Veeranala, Xinyu Zhang, Lilin Jin

## Overview

This project develops a reliable, deep learning-based subtitle segmentation solution 
to improve video accessibility and comprehension. Our work focuses on leveraging 
advanced neural network architectures to accurately segment subtitles, ensuring 
they are properly aligned with spoken content and displayed coherently on the screen. 
This solution is particularly aimed at academic institutions, conferences, and any 
scenario where resources for proprietary subtitle segmentation tools are limited.

## Features (We are still working on this!)

:white_check_mark: **Subtitle Segmentation**: Splits transcribed text into appropriately timed subtitle
  blocks and lines using deep learning models.
  
:white_check_mark: **Open Source**: Committed to the open-source community, all code and models are
  freely available for use and modification.
  
:white_medium_square: **Multi-Language Support**: Designed to work across various languages,
  enhancing the accessibility of diverse video content.
  
:white_medium_square: **User-friendly Tool**: Offers a straightforward interface for uploading video files or YouTube links and receiving segmented subtitles.


## Installation

To set up the project on your local machine, follow these steps:

1. Clone the repository.
2. Install required dependencies.
3. Prepare your video and convert it to audio using any online services.
4. Generate plain text file with `<eol>` and `<eob>` labels:
   - Download Subtitle Segmentation Model [Model link](https://drive.google.com/uc?export=download&id=1-0Jop8pckqJpn4IfDX9KPEsD5uZfSrT2)
   - Run `model1_test_course.ipynb`
6. Add timestamp and convert to `.srt` file:
   - Run notebook `convert_to_srt.ipynb` on Windows or Linux OS, or run it on [google colab](https://colab.research.google.com/drive/1H5y1eP53-73TclaAhfctGhe9FpWXk6w4?usp=sharing)
   - Navigate to the last two cells of the notebook.
   - Replace the example filename with the name of your file in these cells. Ensure the file path and extension are correct.
   - Execute the notebook: Run all the cells in the notebook to perform the conversion.

## Data

Our models are trained on the MuST-Cinema dataset, enriched with markers for 
subtitle segmentation. The dataset is available under the CC BY-NC-ND 4.0 license 
and comprises multilingual speech translation triplets with special symbols for 
subtitle breaks.

You can find the dataset in `\data` folder.

## Evaluation

We conducted manual evaluations. 

Our model was evaluated against the popular video editor CapCut, specifically focusing on the subtitles generated for the DS598 [lecture1](https://drive.google.com/file/d/1j9eu8vJ7uBAKmb2kaL-xNqey87f0PXiG/view?usp=share_link) and [lecture2](). The comparisons were centered around three critical aspects of subtitle quality: timing accuracy, readability, and content accuracy. Both subtitle has a very high accuracy, but our subtitle model demonstrates superior timing accuracy in synchronizing subtitles with the corresponding audio. The readability of subtitles produced by our model was found to be significantly higher than those generated by CapCut.

You can find the subtitle files in `\demo` folder.

## Contributions

We welcome contributions from the community. If you're interested in improving 
the project or have suggestions, please feel free to fork the repository, 
make changes, and submit a pull request.


