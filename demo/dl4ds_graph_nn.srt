1
00:00:00,000 --> 00:00:10,560
All right, so today we're back 
on the book chapter.

2
00:00:10,560 --> 00:00:14,280
I think the treatment 
of graphenolormat 
works was just fine,

3
00:00:14,280 --> 00:00:21,920
so I actually follow it very closely.

4
00:00:21,920 --> 00:00:26,120
And it's more, I think, 
straightforward extension

5
00:00:26,120 --> 00:00:28,040
from what we've been talking about earlier,

6
00:00:28,040 --> 00:00:36,080
opposed to the, oops, I forgot to move the, 
you are here pointer.

7
00:00:36,080 --> 00:00:39,320
But yeah, a little bit more straightforward extension

8
00:00:39,320 --> 00:00:41,200
from the neural net stuff,

9
00:00:41,200 --> 00:00:47,920
from the GANS and diffusion 
models and VAE.

10
00:00:47,920 --> 00:00:54,240
So I think, so I will keep 
the final report and repo here.

11
00:00:54,240 --> 00:00:58,920
I guess technically, I think, 
Osama, you're right,

12
00:00:58,920 --> 00:01:00,400
because there's no final.

13
00:01:00,400 --> 00:01:06,240
You know, they want me to start 
doing grades after the last class,

14
00:01:06,240 --> 00:01:10,240
but it seems like it's not 
strictly enforced.

15
00:01:10,240 --> 00:01:18,920
And so this gives you some more 
time 
to clean things up.

16
00:01:18,920 --> 00:01:24,280
See why it's not.

17
00:01:24,280 --> 00:01:27,520
So I only have two volunteers 
for the 25th.

18
00:01:27,520 --> 00:01:29,520
Thank you to the both of you.

19
00:01:29,520 --> 00:01:38,360
So tonight I'm going to randomly 
select 
the other six, I guess, for the 25th.

20
00:01:38,360 --> 00:01:42,920
And then everyone else 
will go on the 30th.

21
00:01:42,920 --> 00:01:44,560
And so I'll let you know.

22
00:01:44,560 --> 00:01:48,360
The other thing is I updated, 
everyone see the final project info.

23
00:01:48,360 --> 00:01:50,760
I updated on the website 
and grade scope.

24
00:01:50,760 --> 00:01:59,480
So I put a little bit 
more 
instruction on the video.

25
00:01:59,480 --> 00:02:05,720
I updated the report template, 
the LaTeX template,

26
00:02:05,720 --> 00:02:11,240
and I put a comment, a little bit more detail 
on the GitHub repo.

27
00:02:11,240 --> 00:02:15,800
So in the report, if you could put 
a link to the repo,

28
00:02:15,800 --> 00:02:16,680
you can put it on grade scope.

29
00:02:16,680 --> 00:02:19,080
I also had it as well.

30
00:02:19,080 --> 00:02:26,040
But the idea is that, I mean, 
like any kind of well-executed project,

31
00:02:26,040 --> 00:02:28,000
ideas to make it reproducible,

32
00:02:28,000 --> 00:02:31,960
and so somebody can come 
and reproduce your work

33
00:02:31,960 --> 00:02:37,200
is the kind of ideal case.

34
00:02:37,200 --> 00:02:45,600
Any questions about final projects, 
presentations?

35
00:02:45,600 --> 00:02:48,080
All right, so let's jump in.

36
00:02:48,080 --> 00:02:50,600
So graph neural networks.

37
00:02:50,600 --> 00:02:53,280
I think this is an interesting space,

38
00:02:53,280 --> 00:02:57,120
not something that I had really 
worked in before.

39
00:02:57,120 --> 00:03:01,280
But like I said, it's a pretty 
straightforward extension.

40
00:03:01,280 --> 00:03:08,480
So the main challenges 
is that the topology of the graph can vary,

41
00:03:08,480 --> 00:03:14,280
the types of connections 
in which nodes are connected,

42
00:03:14,280 --> 00:03:18,880
the size of these graphs 
can be quite large.

43
00:03:18,880 --> 00:03:23,640
And we have to deal with a case 
where you have a single monolithic graph,

44
00:03:23,640 --> 00:03:28,480
which is, I'll show a bunch of examples, 
but that's a pretty common case.

45
00:03:28,480 --> 00:03:35,440
And so we'll talk about 
what does it mean to do batches of data

46
00:03:35,440 --> 00:03:41,760
because you need that for stochastic 
gradient descent, for example.

47
00:03:41,760 --> 00:03:43,840
So I'll cover definitions.

48
00:03:43,840 --> 00:03:48,400
We'll talk about representation, 
the so-called adjacency matrix,

49
00:03:48,400 --> 00:03:52,320
which is kind of crucial 
for describing the topology of the graph,

50
00:03:52,320 --> 00:03:55,240
and then get into the graph 
neural networks,

51
00:03:55,240 --> 00:03:58,880
how you build a convolutional layer 
on top of a graph,

52
00:03:58,880 --> 00:04:05,440
and then how you build graph classification 
and regression

53
00:04:05,440 --> 00:04:08,280
at the graph level 
and the node level.

54
00:04:08,280 --> 00:04:13,680
Yeah, I just realized that I had also mentioned

55
00:04:13,680 --> 00:04:20,520
we're going to do 
a little recap of the project.

56
00:04:20,520 --> 00:04:24,640
Maybe we'll push it to Thursday.

57
00:04:24,640 --> 00:04:26,720
Okay.

58
00:04:26,720 --> 00:04:30,640
All right, so basic definition 
and examples.

59
00:04:30,640 --> 00:04:38,840
So generally, so the nodes, 
sometimes they're called vertices,

60
00:04:38,840 --> 00:04:40,960
are connected by edges or links.

61
00:04:40,960 --> 00:04:42,480
You can have undirected

62
00:04:42,480 --> 00:04:47,200
where there's no kind of implied direction 
between the nodes,

63
00:04:47,200 --> 00:04:50,880
or you can have directed.

64
00:04:50,880 --> 00:04:52,680
And if you do have a directed connection,

65
00:04:52,680 --> 00:04:56,800
if there's no cycles, 
nothing going back upon itself,

66
00:04:56,800 --> 00:04:58,960
then you can have a directed 
acyclic graph.

67
00:04:58,960 --> 00:05:04,880
And so sometimes you'll hear the term DAG

68
00:05:04,880 --> 00:05:10,040
because there's certain things 
that you can do with DAGs

69
00:05:10,040 --> 00:05:14,560
if they have those two properties.

70
00:05:14,560 --> 00:05:16,080
So we have been looking at,

71
00:05:16,080 --> 00:05:19,960
we didn't really treat it 
as a graph per se,

72
00:05:19,960 --> 00:05:25,600
but we have been looking 
at directed acyclic graphs.

73
00:05:25,600 --> 00:05:31,920
Just the feed-forward neural network 
is one example.

74
00:05:31,920 --> 00:05:35,480
But it's, you know, I guess 
kind of a regular topology.

75
00:05:35,480 --> 00:05:38,160
We didn't treat it as a graph.

76
00:05:38,160 --> 00:05:43,120
I also mentioned this in the VAE talk,

77
00:05:43,120 --> 00:05:46,760
these Bayesian probabilistic 
graphical models

78
00:05:46,760 --> 00:05:48,680
that you can represent something like this,

79
00:05:48,680 --> 00:05:54,520
where you have like a node 
that represents a variable.

80
00:05:54,520 --> 00:05:59,960
And so typically you have something 
like a prior probability

81
00:05:59,960 --> 00:06:03,240
with a directed edge 
to another variable.

82
00:06:03,240 --> 00:06:07,240
And then that node is representing 
the conditional probability,

83
00:06:07,240 --> 00:06:11,480
the probability that this takes on some value

84
00:06:11,480 --> 00:06:17,160
given some latent variable in this case.

85
00:06:17,160 --> 00:06:21,040
So in the book there's a bunch 
of examples as well.

86
00:06:21,040 --> 00:06:22,960
So road networks are graphs

87
00:06:22,960 --> 00:06:27,320
where the physical locations 
or the landmarks are the nodes.

88
00:06:27,320 --> 00:06:29,800
Roads are the edges.

89
00:06:29,800 --> 00:06:34,640
Chemical molecules are examples 
where the nodes are atoms

90
00:06:34,640 --> 00:06:38,080
and the edges are chemical bonds between them.

91
00:06:38,080 --> 00:06:42,480
You can have electrical circuit as graphs.

92
00:06:42,480 --> 00:06:46,400
The nodes are components 
or the junctions.

93
00:06:46,400 --> 00:06:53,680
And then the edges 
are the wires connecting them.

94
00:06:53,680 --> 00:06:55,520
Some other examples.

95
00:06:55,520 --> 00:06:56,600
Social networks,

96
00:06:56,600 --> 00:07:00,400
and this is where 
it can be a very large graph.

97
00:07:00,400 --> 00:07:04,000
So you can take all of Facebook 
as a social network.

98
00:07:04,000 --> 00:07:05,200
The nodes are the people

99
00:07:05,200 --> 00:07:10,760
and the edges are people 
who have friended each other on the platform.

100
00:07:10,760 --> 00:07:12,600
Scientific literature,

101
00:07:12,600 --> 00:07:14,680
the nodes 
are the papers, citations,

102
00:07:14,680 --> 00:07:19,160
so these are directed, generally acyclic,

103
00:07:19,160 --> 00:07:21,640
where it's citing password.

104
00:07:21,640 --> 00:07:27,080
And then there's, I'm not aware 
of actually much use here

105
00:07:27,080 --> 00:07:28,600
of something like a knowledge graph,

106
00:07:28,600 --> 00:07:31,960
but you can have this kind of hybrid graph.

107
00:07:31,960 --> 00:07:35,840
Actually, in this case, yeah, 
these are all directed.

108
00:07:35,840 --> 00:07:38,000
But it is acyclic.

109
00:07:38,000 --> 00:07:41,520
Actually, in the book I think 
it says 
it's acyclic,

110
00:07:41,520 --> 00:07:42,560
if I remember right,

111
00:07:42,560 --> 00:07:46,320
but I see one cycle right there.

112
00:07:46,320 --> 00:07:52,200
And actually, yeah.

113
00:07:52,200 --> 00:07:54,360
So in this case, you're trying to,

114
00:07:54,360 --> 00:07:57,920
the edges correspond to kind of relationships

115
00:07:57,920 --> 00:08:00,080
between entities that are the nodes.

116
00:08:00,080 --> 00:08:05,320
So you can also, things 
that are maybe not,

117
00:08:05,320 --> 00:08:07,760
you wouldn't typically think of as graphs,

118
00:08:07,760 --> 00:08:11,600
but if you have a 3D geometry 
of a physical object,

119
00:08:11,600 --> 00:08:13,840
like in this case a plane,

120
00:08:13,840 --> 00:08:17,920
the vertices in 3D 
can be the nodes

121
00:08:17,920 --> 00:08:18,840
and then they could just,

122
00:08:18,840 --> 00:08:28,400
you could just have connections 
to all the nodes nearby.

123
00:08:28,400 --> 00:08:36,600
There's also, there's a scene graph, 
which can be used in 3D rendering,

124
00:08:36,600 --> 00:08:42,520
but it's typically, it describes 
the relationship of 3D objects

125
00:08:42,520 --> 00:08:43,720
and these can be hierarchical.

126
00:08:43,720 --> 00:08:46,720
So you can have, in this case, 
like a room description

127
00:08:46,720 --> 00:08:51,880
where you have the connection 
between

128
00:08:51,880 --> 00:08:55,720
like the back walls connected 
to the right wall and the left wall

129
00:08:55,720 --> 00:08:57,920
and the ceiling and the floor.

130
00:08:57,920 --> 00:09:02,280
And then you have a hierarchy, 
so you can then have a lampshade,

131
00:09:02,280 --> 00:09:04,520
which is broken further down into components

132
00:09:04,520 --> 00:09:08,640
or you can have the table, 
which is broken further down as well.

133
00:09:08,640 --> 00:09:14,520
I put some links of papers that,

134
00:09:14,520 --> 00:09:18,840
some of them are applying neural networks

135
00:09:18,840 --> 00:09:21,440
to these kinds of scene graphs.

136
00:09:21,440 --> 00:09:23,440
And then there's a bunch 
of other examples.

137
00:09:23,440 --> 00:09:27,800
So Wikipedia, where articles 
are the nodes

138
00:09:27,800 --> 00:09:30,000
and the hyperlinks are the edges.

139
00:09:30,000 --> 00:09:33,760
Computer programs, 
you can show as graphs.

140
00:09:33,760 --> 00:09:36,640
In fact, in the example that I did

141
00:09:36,640 --> 00:09:40,360
with the Jupiter notebook 
for the micrograd,

142
00:09:40,360 --> 00:09:43,640
we were basically building 
a tensor compute graph,

143
00:09:43,640 --> 00:09:46,760
right, if you remember.

144
00:09:46,760 --> 00:09:53,400
So, but yeah, so proteins can be,

145
00:09:53,400 --> 00:09:54,880
the proteins are the nodes,

146
00:09:54,880 --> 00:09:58,880
the edges are where the proteins interact,

147
00:09:58,880 --> 00:10:02,720
and then you can even have 
like sets 
or lists

148
00:10:02,720 --> 00:10:04,040
or even images

149
00:10:04,040 --> 00:10:11,480
where you just have 
the six or eight neighboring pixels

150
00:10:11,480 --> 00:10:18,920
just connected to a particular pixel 
as well, okay?

151
00:10:18,920 --> 00:10:22,800
All right, so how do we describe these?

152
00:10:22,800 --> 00:10:29,440
Typically, we're going to define something 
called like a node embedding.

153
00:10:29,440 --> 00:10:32,360
So we talked about embedding 
vectors 
with transformers.

154
00:10:32,360 --> 00:10:37,960
There's actually some analogies there, 
which we'll see later,

155
00:10:37,960 --> 00:10:43,080
but you can have some kind of information 
that you have

156
00:10:43,080 --> 00:10:47,680
basically translated 
into an embedding vector, for example.

157
00:10:47,680 --> 00:10:51,240
Typically, they're all 
of the same dimension.

158
00:10:51,240 --> 00:10:53,200
You can also do the same thing with edges.

159
00:10:53,200 --> 00:10:58,320
So you can have an edge embedding 
or embedding vector for every edge.

160
00:10:58,320 --> 00:11:04,760
And then typically, 
we'll define 
an adjacency matrix.

161
00:11:04,760 --> 00:11:06,200
And the way you read this

162
00:11:06,200 --> 00:11:09,200
is that this is just, in this case,

163
00:11:09,200 --> 00:11:11,440
for undirected graphs,

164
00:11:11,440 --> 00:11:13,280
that you just read the indices.

165
00:11:13,280 --> 00:11:18,920
And so if there's a non-zero, 
for example, here at 3, 1,

166
00:11:18,920 --> 00:11:24,320
that means there's a connection 
between 3 and 1 on the nodes.

167
00:11:24,320 --> 00:11:26,280
And because it's undirected,

168
00:11:26,280 --> 00:11:29,320
then you would expect 
there's going 
to be a connection on 1, 3.

169
00:11:29,320 --> 00:11:34,120
So it's going to be symmetric, 
always a symmetric matrix,

170
00:11:34,120 --> 00:11:36,680
but the diagonal is typically zero

171
00:11:36,680 --> 00:11:40,200
because there's no, at least, external edge

172
00:11:40,200 --> 00:11:42,040
connecting back to itself.

173
00:11:42,040 --> 00:11:48,120
So you see the diagonals are zeros, 
and then we have non-zeros,

174
00:11:48,120 --> 00:11:53,680
actually typically just put a 1 
where there's a connection.

175
00:11:53,680 --> 00:11:55,800
And these can be very sparse,

176
00:11:55,800 --> 00:12:02,280
but sometimes you'll use sparse 
representation 
for the matrices.

177
00:12:02,280 --> 00:12:06,360
Okay, so we could take 
all of the node data edges,

178
00:12:06,360 --> 00:12:10,880
and then we can put them 
into a matrix, right?

179
00:12:10,880 --> 00:12:14,760
So remember, we have 
a d-dimensional 
embedding vector

180
00:12:14,760 --> 00:12:16,640
for every node,

181
00:12:16,640 --> 00:12:19,680
and we have, let's say, n nodes.

182
00:12:19,680 --> 00:12:23,320
In this case, yeah, 
it lines up six nodes.

183
00:12:23,320 --> 00:12:28,400
And so just like we were doing 
for transformers,

184
00:12:28,400 --> 00:12:32,000
you can put together an input matrix,

185
00:12:32,000 --> 00:12:34,480
that is just d-dimension 
and then n columns,

186
00:12:34,480 --> 00:12:36,200
one column for every node.

187
00:12:36,200 --> 00:12:39,040
Okay?

188
00:12:39,040 --> 00:12:43,440
And then the same thing 
for the edges.

189
00:12:43,440 --> 00:12:48,760
You can have a different 
dimensionality 
to the edges,

190
00:12:48,760 --> 00:12:54,880
but they represent each one of these edges.

191
00:12:54,880 --> 00:13:00,600
And so there's, you know, it's probably 
some kind of metadata here,

192
00:13:00,600 --> 00:13:03,600
but these are the connections 
that they represent.

193
00:13:03,600 --> 00:13:17,440
Okay? Okay, so let's look at some properties 
of the adjacency matrix.

194
00:13:17,440 --> 00:13:20,240
In this case, this is a simple 
undirected graph

195
00:13:20,240 --> 00:13:22,880
with eight nodes.

196
00:13:22,880 --> 00:13:28,840
And so you can, I guess 
the font's a little bit small,

197
00:13:28,840 --> 00:13:35,080
but you can convince yourself 
that there's a one everywhere

198
00:13:35,080 --> 00:13:41,880
where there's a connection on the graph.

199
00:13:41,880 --> 00:13:45,920
And so we can describe one of the nodes

200
00:13:45,920 --> 00:13:48,680
as just a one-hot encoded vector, right?

201
00:13:48,680 --> 00:13:54,720
And so like node six 
is just a vector,

202
00:13:54,720 --> 00:13:57,320
the length, this should be length eight,

203
00:13:57,320 --> 00:13:59,360
and then there's just a,

204
00:13:59,360 --> 00:14:01,120
it's all zeros except a one

205
00:14:01,120 --> 00:14:04,280
at the sixth position there, right?

206
00:14:04,280 --> 00:14:11,040
So, and then once you do that,

207
00:14:11,040 --> 00:14:15,400
you can left multiply the embedding vector,

208
00:14:15,400 --> 00:14:18,840
the one-hot vector here 
by the adjacency matrix.

209
00:14:18,840 --> 00:14:23,240
And what that gives you, 
just think about it, right?

210
00:14:23,240 --> 00:14:25,680
You're taking each row 
and then 
you're multiplying here.

211
00:14:25,680 --> 00:14:29,520
So you're just pulling out 
the whatever column is non-zero.

212
00:14:29,520 --> 00:14:34,800
In this case, you're just pulling 
out 
the sixth column here.

213
00:14:34,800 --> 00:14:36,360
And remember on the adjacency matrix,

214
00:14:36,360 --> 00:14:39,240
this is saying that for the sixth node,

215
00:14:39,240 --> 00:14:45,400
these are the nodes 
that it's connected to, right?

216
00:14:45,400 --> 00:14:48,240
And so it's telling you 
that these are,

217
00:14:48,240 --> 00:14:50,000
just by doing this multiplication,

218
00:14:50,000 --> 00:14:53,520
it's telling you those are the neighboring nodes

219
00:14:53,520 --> 00:14:56,840
to the node that we're interested in.

220
00:14:56,840 --> 00:14:58,200
And then you can do it again.

221
00:14:58,200 --> 00:15:06,200
You can left multiply again 
by another A adjacency matrix.

222
00:15:06,200 --> 00:15:11,240
And now what it's showing you 
is that all of the nodes

223
00:15:11,240 --> 00:15:14,640
is you can reach in exactly two steps.

224
00:15:14,640 --> 00:15:20,240
And you can hit a node twice,

225
00:15:20,240 --> 00:15:26,640
but you see like seven is not lit up, 
it's not colored

226
00:15:26,640 --> 00:15:32,120
because you can't get there 
in exactly two steps.

227
00:15:32,120 --> 00:15:34,080
It's not up to two steps,

228
00:15:34,080 --> 00:15:43,240
it's exactly two steps 
that it's describing.

229
00:15:43,240 --> 00:15:45,560
Okay?

230
00:15:45,560 --> 00:15:49,600
And so if you just multiply A by itself,

231
00:15:49,600 --> 00:15:55,400
what this is telling you 
is that,

232
00:15:55,400 --> 00:16:01,080
I write it here,

233
00:16:01,080 --> 00:16:04,640
that if you raise A to whatever power, 
for example of two,

234
00:16:04,640 --> 00:16:09,200
just multiply it by itself 
that many times,

235
00:16:09,200 --> 00:16:11,800
then every position here,

236
00:16:11,800 --> 00:16:13,880
it contains the number of, as it says here,

237
00:16:13,880 --> 00:16:21,240
unique walks of that length 
that you raise the power to,

238
00:16:21,240 --> 00:16:24,960
the number of unique walks 
from node N to N.

239
00:16:24,960 --> 00:16:36,520
So if you look here, let's say one, two, three, four, one, two, three,

240
00:16:36,520 --> 00:16:42,080
well, okay, that's probably, well, yeah, this is saying 
from four to four,

241
00:16:42,080 --> 00:16:47,920
you can, there should be 
four unique walks,

242
00:16:47,920 --> 00:16:49,720
steps of two, right?

243
00:16:49,720 --> 00:16:54,600
Is that, I don't see the fourth one.

244
00:16:54,600 --> 00:16:55,360
Does anyone see the fourth one?

245
00:16:55,360 --> 00:17:00,680
Oh, it's three, three, okay, yeah.

246
00:17:00,680 --> 00:17:05,880
Okay, yeah, and so the,

247
00:17:05,880 --> 00:17:07,240
but the thing to remember here

248
00:17:07,240 --> 00:17:10,280
is that this, that value,

249
00:17:10,280 --> 00:17:17,560
that it's not the number 
of unique paths,

250
00:17:17,560 --> 00:17:19,800
so you can visit the same node,

251
00:17:19,800 --> 00:17:22,840
but what you can think about 
is kind of an upper bound,

252
00:17:22,840 --> 00:17:28,000
but it means that there is some path 
up to the value here

253
00:17:28,000 --> 00:17:31,120
that, with the connection.

254
00:17:31,120 --> 00:17:35,960
So there can be different distance 
walks between the nodes.

255
00:17:35,960 --> 00:17:39,640
Okay.

256
00:17:39,640 --> 00:17:46,280
Okay, so now you can do, 
you can do some things,

257
00:17:46,280 --> 00:17:50,280
like for example, I just created 
a tiny graph

258
00:17:50,280 --> 00:17:52,280
with these node embeddings,

259
00:17:52,280 --> 00:17:54,480
three dimensional node embeddings,

260
00:17:54,480 --> 00:17:59,960
which you can put together 
into a node data matrix,

261
00:17:59,960 --> 00:18:06,000
and then this should be the adjacency matrix 
for that, for that graph.

262
00:18:06,000 --> 00:18:11,360
You can define a permutation matrix,

263
00:18:11,360 --> 00:18:17,240
and the permutation matrix 
just has a one in each column and each row,

264
00:18:17,240 --> 00:18:23,440
so you can't have more than one, 
in a column or a row,

265
00:18:23,440 --> 00:18:28,000
but if you take that data matrix 
and you right multiply it,

266
00:18:28,000 --> 00:18:31,440
you're basically just permuting these, right?

267
00:18:31,440 --> 00:18:36,280
So, same thing again, 
you're just taking the rows

268
00:18:36,280 --> 00:18:40,040
and you're just grabbing 
for each of these columns,

269
00:18:40,040 --> 00:18:44,760
you're just grabbing one of the columns out,

270
00:18:44,760 --> 00:18:49,000
and so this permutation matrix here

271
00:18:49,000 --> 00:18:53,040
is just permuting the values of the data matrix there,

272
00:18:53,040 --> 00:18:59,880
and so you can do the same thing to the adjacency matrix,

273
00:18:59,880 --> 00:19:03,400
but you have to post multiply and premultiply

274
00:19:03,400 --> 00:19:05,360
by the transpose,

275
00:19:05,360 --> 00:19:10,120
and so you can, hopefully I did this right,

276
00:19:10,120 --> 00:19:11,840
so I permuted it,

277
00:19:11,840 --> 00:19:13,960
I left the old indices,

278
00:19:13,960 --> 00:19:14,800
these are the new indices,

279
00:19:14,800 --> 00:19:17,280
I renumbered them,

280
00:19:17,280 --> 00:19:19,560
they're just different numbers,

281
00:19:19,560 --> 00:19:21,840
but they still have the same value,

282
00:19:21,840 --> 00:19:28,360
they're the same topology, 
they're just indexed differently,

283
00:19:28,360 --> 00:19:34,760
but the new adjacency matrix 
should be correct for this.

284
00:19:34,760 --> 00:19:46,120
So, yeah, let me talk 
about some properties

285
00:19:46,120 --> 00:19:50,520
and then we'll go back 
to the permutation 
adjacency matrix.

286
00:19:50,520 --> 00:19:54,160
So, how do we extend this 
to a graph neural network?

287
00:19:54,160 --> 00:19:56,320
Essentially what we're going to do 
is we're going to take

288
00:19:56,320 --> 00:20:01,000
those node embedding matrices 
that I showed you

289
00:20:01,000 --> 00:20:04,760
and the adjacency matrix 
which describes the topology of the graph,

290
00:20:04,760 --> 00:20:10,400
and we're going to pass them 
through layers of a neural network.

291
00:20:10,400 --> 00:20:14,520
So, the node embeddings 
are updated at each layer,

292
00:20:14,520 --> 00:20:17,880
so you have these kind of hidden representation

293
00:20:17,880 --> 00:20:23,800
or hidden activations 
after each layer,

294
00:20:23,800 --> 00:20:29,280
and of course when you start 
the input data

295
00:20:29,280 --> 00:20:34,480
is just the embedding vector 
of each node,

296
00:20:34,480 --> 00:20:39,280
but at the end you have this output

297
00:20:39,280 --> 00:20:44,520
which is, you know, you have a representation 
for each node,

298
00:20:44,520 --> 00:20:49,000
but now it's incorporated context, you know,

299
00:20:49,000 --> 00:20:51,080
from its neighbors in the rest of the graph

300
00:20:51,080 --> 00:20:56,160
because as you'll see, well, we're going to kind of aggregate information

301
00:20:56,160 --> 00:20:59,320
from the neighboring nodes.

302
00:20:59,320 --> 00:21:02,040
And so, yeah, interestingly,

303
00:21:02,040 --> 00:21:06,080
this is kind of like word embeddings 
in a transformer,

304
00:21:06,080 --> 00:21:09,280
you know, when you start out you just have 
a unique word embedding

305
00:21:09,280 --> 00:21:11,360
for every word or token,

306
00:21:11,360 --> 00:21:15,160
but as it travels through the transformer

307
00:21:15,160 --> 00:21:20,000
that what you have at the end 
is you have, you know,

308
00:21:20,000 --> 00:21:23,880
kind of something 
that represents the word in its context

309
00:21:23,880 --> 00:21:30,520
to the other words in the sentence.

310
00:21:30,520 --> 00:21:34,640
Okay.

311
00:21:34,640 --> 00:21:41,040
Okay, so you can do, 
just like other neural networks,

312
00:21:41,040 --> 00:21:45,600
we can create classification 
and regression networks with these,

313
00:21:45,600 --> 00:21:48,720
and so, you know, 
some examples, for example,

314
00:21:48,720 --> 00:21:51,320
you can give it a molecular structure 
and graph form

315
00:21:51,320 --> 00:21:55,640
and then you can try to figure out 
if it's poisonous or not

316
00:21:55,640 --> 00:21:58,800
based on the chemical structure,

317
00:21:58,800 --> 00:22:02,200
or you can regress boiling 
and freezing points,

318
00:22:02,200 --> 00:22:08,920
so two regression values 
out of a molecular graph.

319
00:22:08,920 --> 00:22:14,440
Yeah, and so generally,

320
00:22:14,440 --> 00:22:18,240
just like 
with other neural networks 
that we saw,

321
00:22:18,240 --> 00:22:21,600
that you can put a head 
to the network, right,

322
00:22:21,600 --> 00:22:26,720
so you can put a classification head 
or a regression head

323
00:22:26,720 --> 00:22:28,280
at the end of the network

324
00:22:28,280 --> 00:22:33,880
to get it into that kind of final regression 
or classification form

325
00:22:33,880 --> 00:22:37,080
and I'll show you some examples.

326
00:22:37,080 --> 00:22:39,400
So I'm going to start 
with just the head of the network

327
00:22:39,400 --> 00:22:44,000
and then I'll build up 
to how you build the network itself,

328
00:22:44,000 --> 00:22:48,880
but imagine you had a graph 
with node embeddings.

329
00:22:48,880 --> 00:22:55,120
We're going to focus mostly on node, 
kind of the node data.

330
00:22:55,120 --> 00:23:01,040
At the very end, I'll show 
there's kind of a simple way

331
00:23:01,040 --> 00:23:02,720
to handle data in the edges as well,

332
00:23:02,720 --> 00:23:04,720
the edge embeddings,

333
00:23:04,720 --> 00:23:07,800
but let's focus on the node embeddings first.

334
00:23:07,800 --> 00:23:11,440
So let's say you put it 
through some neural network,

335
00:23:11,440 --> 00:23:12,960
which we haven't defined,

336
00:23:12,960 --> 00:23:19,320
but you have, on the output, 
you're going to have some, 
you know,

337
00:23:19,320 --> 00:23:21,000
activations or embeddings

338
00:23:21,000 --> 00:23:24,200
associated with each node

339
00:23:24,200 --> 00:23:26,840
and then you can combine it.

340
00:23:26,840 --> 00:23:29,720
You can put it through a linear layer

341
00:23:29,720 --> 00:23:33,240
and then you can put it 
through, 
for example,

342
00:23:33,240 --> 00:23:34,200
a sigmoid

343
00:23:34,200 --> 00:23:36,600
or a softmax 
or something like that

344
00:23:36,600 --> 00:23:40,320
and then you can end up 
with probabilities 
for every class.

345
00:23:40,320 --> 00:23:43,440
So here's an example 
for just binary classification.

346
00:23:43,440 --> 00:23:48,880
It's just kind of what should 
look 
pretty familiar.

347
00:23:48,880 --> 00:23:53,400
You have the sigmoid, 
you have a beta, a bias here,

348
00:23:53,400 --> 00:23:55,560
and then you have the weights

349
00:23:55,560 --> 00:24:00,840
and this one is just a d-dimensional row vector

350
00:24:00,840 --> 00:24:07,920
and then you have the, you know, 
your embeddings matrix here

351
00:24:07,920 --> 00:24:10,040
and so this is just multiplying every,

352
00:24:10,040 --> 00:24:13,960
it's just weighting every column, right,

353
00:24:13,960 --> 00:24:16,600
of your embedding matrix.

354
00:24:16,600 --> 00:24:23,560
But then in this case, because 
we want 
to get a single output,

355
00:24:23,560 --> 00:24:25,160
we have this one vector, right?

356
00:24:25,160 --> 00:24:33,080
This is just a, I think this is a, yeah, typo here.

357
00:24:33,080 --> 00:24:37,280
This is just a column of ones, right?

358
00:24:37,280 --> 00:24:41,040
And this has the effect 
of basically summing,

359
00:24:41,040 --> 00:24:47,800
well, because we're dividing 
by the number of vectors,

360
00:24:47,800 --> 00:24:55,760
this has the, basically, 
it's just taking the average 
of every one 
of those vectors and so,

361
00:24:55,760 --> 00:25:00,480
and collapsing it into a single value.

362
00:25:00,480 --> 00:25:09,880
And so this is like mean pooling, 
like we showed you before, early.

363
00:25:09,880 --> 00:25:19,600
Okay, and so that was at the graph level.

364
00:25:19,600 --> 00:25:26,760
So we collapsed everything 
into a single value.

365
00:25:26,760 --> 00:25:29,840
But you can also do it 
at the node level.

366
00:25:29,840 --> 00:25:32,840
So, for example, 
you can do 
binary classification,

367
00:25:32,840 --> 00:25:38,480
you can decide between, let's say, 
one of two kinds of nodes here

368
00:25:38,480 --> 00:25:40,120
and it's very similar

369
00:25:40,120 --> 00:25:44,880
except you're doing it 
on every embedding vector.

370
00:25:44,880 --> 00:25:45,720
This is not the matrix,

371
00:25:45,720 --> 00:25:47,920
the whole matrix 
of all the embedding vectors stacked.

372
00:25:47,920 --> 00:25:51,040
This is just the individual node vector

373
00:25:51,040 --> 00:25:56,160
indexed by the node number n.

374
00:25:56,160 --> 00:26:01,720
And then so you're doing that now 
and you're producing a vector

375
00:26:01,720 --> 00:26:08,360
and then you have a, actually, this is a scale

376
00:26:08,360 --> 00:26:10,120
or you're adding the bias

377
00:26:10,120 --> 00:26:12,760
and then you're getting the value 
just for that particular node.

378
00:26:12,760 --> 00:26:20,400
Okay, does that make sense?

379
00:26:20,400 --> 00:26:25,880
Okay, yeah, so here's an example 
where you can decide,

380
00:26:25,880 --> 00:26:29,360
let's say you have,

381
00:26:29,360 --> 00:26:32,680
in some cases, 
maybe an edge doesn't exist

382
00:26:32,680 --> 00:26:35,840
and you're trying to predict 
if an edge should exist there.

383
00:26:35,840 --> 00:26:39,160
And so here you can take 
the, essentially,

384
00:26:39,160 --> 00:26:40,920
the dot product

385
00:26:40,920 --> 00:26:45,840
of the neighboring node 
embedding vectors,

386
00:26:45,840 --> 00:26:47,480
send it through a sigmoid

387
00:26:47,480 --> 00:26:51,160
and then you can train it 
to basically give you the probability

388
00:26:51,160 --> 00:26:54,240
that, in fact, this edge should be there.

389
00:26:54,240 --> 00:26:57,840
It's kind of another example.

390
00:26:57,840 --> 00:27:08,000
Okay? Okay, so let's build up the, 
those were the heads of the graph,

391
00:27:08,000 --> 00:27:12,040
let's build up the body 
of the graph itself.

392
00:27:12,040 --> 00:27:15,320
Just like we saw before when we were,

393
00:27:15,320 --> 00:27:16,920
it seemed like a long time ago now,

394
00:27:16,920 --> 00:27:20,160
when we were building up 
the deep neural networks,

395
00:27:20,160 --> 00:27:24,320
that we're just going to create 
all of these hidden layers.

396
00:27:24,320 --> 00:27:25,480
We're going to start with the input.

397
00:27:25,480 --> 00:27:28,240
We now have the adjacency matrix,

398
00:27:28,240 --> 00:27:29,800
but we have parameters for each layer.

399
00:27:29,800 --> 00:27:32,200
We're just going to have 
a bunch of hidden layers

400
00:27:32,200 --> 00:27:33,440
and then we're going to, you know,

401
00:27:33,440 --> 00:27:36,560
end up with that final hidden layer

402
00:27:36,560 --> 00:27:39,800
before we get to the head 
of the network.

403
00:27:39,800 --> 00:27:44,440
And so we're going to define 
what this function f is

404
00:27:44,440 --> 00:27:49,960
to create the next hidden layer.

405
00:27:49,960 --> 00:27:59,840
Okay, so just like 
in the convolutional networks

406
00:27:59,840 --> 00:28:03,280
and some of the other networks,

407
00:28:03,280 --> 00:28:08,200
that we do care about equivalence 
and invariance.

408
00:28:08,200 --> 00:28:11,320
And so remember, equivalence 
means that if I do,

409
00:28:11,320 --> 00:28:15,520
if I transform the input somehow,

410
00:28:15,520 --> 00:28:17,200
that the output is transformed 
the same way.

411
00:28:17,200 --> 00:28:21,360
And so remember back 
to the permutation matrix,

412
00:28:21,360 --> 00:28:24,840
I can right multiply my data matrix

413
00:28:24,840 --> 00:28:27,560
by the permutation back to left

414
00:28:27,560 --> 00:28:31,080
multiply and right multiply 
the adjacency matrix.

415
00:28:31,080 --> 00:28:35,560
But if I do that, then the output 
is just the hidden layer

416
00:28:35,560 --> 00:28:38,320
with the same permutation, right,

417
00:28:38,320 --> 00:28:41,320
multiplied by the same permutation matrix.

418
00:28:41,320 --> 00:28:47,720
And so it's equivariant to permutation.

419
00:28:47,720 --> 00:28:54,320
And then invariant, if we're doing 
something like sigmoid,

420
00:28:54,320 --> 00:28:58,440
since we're summing 
all of these up anyway,

421
00:28:58,440 --> 00:29:05,560
we don't really care 
if we're shuffling 
the order of the nodes.

422
00:29:05,560 --> 00:29:09,600
And so in this case, if we're doing 
like graph level classification,

423
00:29:09,600 --> 00:29:16,840
it's invariant to permutation as well.

424
00:29:16,840 --> 00:29:28,640
So that's a valuable thing 
for the graph neural networks.

425
00:29:28,640 --> 00:29:32,720
Okay, so let's now build out 
a convolutional layer for the graph.

426
00:29:32,720 --> 00:29:36,160
And so I'm going to define a function here,

427
00:29:36,160 --> 00:29:38,040
the aggregation function.

428
00:29:38,040 --> 00:29:42,560
And essentially what it's doing, 
you can see here

429
00:29:42,560 --> 00:29:51,760
that for a particular node 
and a particular layer,

430
00:29:51,760 --> 00:29:56,000
okay, that I'm just taking the neighborhood.

431
00:29:56,000 --> 00:30:00,120
So NE is just saying 
that it's returning the indices

432
00:30:00,120 --> 00:30:04,280
of all the neighboring nodes 
for the node,

433
00:30:04,280 --> 00:30:09,400
not including itself.

434
00:30:09,400 --> 00:30:15,760
Okay, so that's going to be handy.

435
00:30:15,760 --> 00:30:23,240
And then the whole layer itself 
is we have a weight matrix.

436
00:30:23,240 --> 00:30:25,280
We're going to multiply it by the node,

437
00:30:25,280 --> 00:30:30,120
the embedding vector 
for the node itself,

438
00:30:30,120 --> 00:30:33,320
but we're also going to multiply it

439
00:30:33,320 --> 00:30:36,600
against the sum of all the neighbors as well.

440
00:30:36,600 --> 00:30:40,000
So it's kind of like the convolution, right?

441
00:30:40,000 --> 00:30:43,240
We're taking weights 
and we're weighting 
all of the neighborhood

442
00:30:43,240 --> 00:30:46,040
around a pixel in that case.

443
00:30:46,040 --> 00:30:49,040
Okay, does that make sense?

444
00:30:49,040 --> 00:30:52,280
And we're using the same weights for both.

445
00:30:52,280 --> 00:30:56,640
So there's already some weight 
sharing happening here.

446
00:30:56,640 --> 00:31:05,360
And those same weights 
are going to be applied

447
00:31:05,360 --> 00:31:08,640
to every node in that graph at that layer.

448
00:31:08,640 --> 00:31:11,240
Just like in a convolutional network.

449
00:31:11,240 --> 00:31:26,680
Okay, so, yeah, so if you have, in this case,

450
00:31:26,680 --> 00:31:28,960
you have your input,

451
00:31:28,960 --> 00:31:32,040
you have, 
after the first layer,

452
00:31:32,040 --> 00:31:35,280
so you do the aggregation 
for each node,

453
00:31:35,280 --> 00:31:39,640
you have an omega-zero weight matrix,

454
00:31:39,640 --> 00:31:41,600
and then put it through an activation

455
00:31:41,600 --> 00:31:44,960
and you get the first hidden unit output,

456
00:31:44,960 --> 00:31:46,920
the embeddings at each node.

457
00:31:46,920 --> 00:31:50,560
And then you can do this 
through a bunch of layers

458
00:31:50,560 --> 00:31:53,160
until you get to the last layer

459
00:31:53,160 --> 00:31:59,000
and then you just do it again 
at the last layer.

460
00:31:59,000 --> 00:32:04,960
So it should look a lot like 
the regular neural nets

461
00:32:04,960 --> 00:32:13,240
and the convolutional neural nets 
that you saw.

462
00:32:13,240 --> 00:32:20,000
Okay, so we have this equation 
where we had the aggregates,

463
00:32:20,000 --> 00:32:21,080
we have this weight matrix,

464
00:32:21,080 --> 00:32:24,200
we applied it to the node 
embedding vector,

465
00:32:24,200 --> 00:32:27,720
the sum of all the neighbors.

466
00:32:27,720 --> 00:32:35,280
And then, so, if you do this 
to the entire,

467
00:32:35,280 --> 00:32:38,240
you can do this to just the embedding 
vector of a node,

468
00:32:38,240 --> 00:32:41,360
but you can also just stack them all together,

469
00:32:41,360 --> 00:32:45,320
like the data matrix 
that I was showing you.

470
00:32:45,320 --> 00:32:47,040
So capital H here.

471
00:32:47,040 --> 00:32:52,000
And what you see is that 
you're applying it to the same,

472
00:32:52,000 --> 00:32:57,880
yeah, so I guess the first thing 
that's important to note here

473
00:32:57,880 --> 00:33:02,760
is that remember when we right-multiplied 
by the adjacency matrix,

474
00:33:02,760 --> 00:33:06,320
we got all of the immediate 
neighbors 
of the node, right?

475
00:33:06,320 --> 00:33:10,560
And so we're replacing 
the aggregate function

476
00:33:10,560 --> 00:33:14,440
with this right-multiply by the adjacency matrix.

477
00:33:14,440 --> 00:33:21,640
And so now we have the hidden 
layer 
embedding matrix

478
00:33:21,640 --> 00:33:22,240
showing up twice

479
00:33:22,240 --> 00:33:24,640
and so we can just factor it out

480
00:33:24,640 --> 00:33:27,640
and then we have the equation here.

481
00:33:27,640 --> 00:33:34,720
And so you can do it in matrix form 
on the whole data matrix.

482
00:33:34,720 --> 00:33:38,600
And then you just have the adjacency

483
00:33:38,600 --> 00:33:45,520
plus this is the identity matrix here.

484
00:33:45,520 --> 00:33:51,320
So the nice thing is that this 
is equivariant to permutations.

485
00:33:51,320 --> 00:33:54,480
It can handle an arbitrary 
number 
of neighbors.

486
00:33:54,480 --> 00:33:58,520
So that's all handled 
in the adjacency matrix.

487
00:33:58,520 --> 00:34:01,840
We don't have to deal with any kind 
of weird indexing or anything.

488
00:34:01,840 --> 00:34:06,360
And it is, you know, 
the adjacency matrix

489
00:34:06,360 --> 00:34:08,520
is describing the graph structure

490
00:34:08,520 --> 00:34:11,840
so it can exploit any kind 
of graph structure.

491
00:34:11,840 --> 00:34:17,200
And then we are sharing parameters 
across every one of the nodes.

492
00:34:17,200 --> 00:34:25,840
So we have that kind of efficiency 
of parameters as well.

493
00:34:25,840 --> 00:34:33,320
Okay, so let's put it all together 
now

494
00:34:33,320 --> 00:34:36,600
in graph and node classification.

495
00:34:36,600 --> 00:34:40,760
And there are, I'm going 
to release a few notebooks.

496
00:34:40,760 --> 00:34:42,000
They're pretty straightforward,

497
00:34:42,000 --> 00:34:45,160
but I think it just helps 
to have a little experience

498
00:34:45,160 --> 00:34:46,400
with the Python code

499
00:34:46,400 --> 00:34:50,920
and the matrices 
and the indexing a little bit.

500
00:34:50,920 --> 00:34:56,760
And so the one example 
you're going 
to see in the Python notebook,

501
00:34:56,760 --> 00:34:59,560
maybe it's, yeah, 13.

502
00:34:59,560 --> 00:35:05,600
2 notebook, is that, so 
you can put it all together.

503
00:35:05,600 --> 00:35:09,720
You can put a sigmoid head on it 
like this with mean pooling.

504
00:35:09,720 --> 00:35:14,400
And then we can do, 
for example, classification.

505
00:35:14,400 --> 00:35:16,480
And so in this case, there's 118 elements.

506
00:35:16,480 --> 00:35:20,720
And so we can just have 
a one-hot encoded

507
00:35:20,720 --> 00:35:22,120
for every node

508
00:35:22,120 --> 00:35:24,160
to describe which element it is.

509
00:35:24,160 --> 00:35:27,840
And then the first parameter,

510
00:35:27,840 --> 00:35:31,480
we can decide what embedding 
dimension we want

511
00:35:31,480 --> 00:35:37,120
so that first weight matrix here 
can just be d by 118.

512
00:35:37,120 --> 00:35:41,240
So we switch to some 
d-dimensional embedding.

513
00:35:41,240 --> 00:35:44,080
And then we have the weight matrix,

514
00:35:44,080 --> 00:35:49,520
which is a 1 by d row vector as well.

515
00:35:49,520 --> 00:35:52,960
But with that, you can put it all together

516
00:35:52,960 --> 00:35:58,080
and now you basically have 
a graph classification network

517
00:35:58,080 --> 00:36:05,000
for, I guess, molecular compound detection.

518
00:36:05,000 --> 00:36:12,680
All right, so let me define 
another term here.

519
00:36:12,680 --> 00:36:21,040
So the stuff we've been talking 
about up to date with supervised learning

520
00:36:21,040 --> 00:36:25,360
has been these inductive kind of models.

521
00:36:25,360 --> 00:36:30,040
And so typically what we do 
is we split our data

522
00:36:30,040 --> 00:36:32,080
into training and test data.

523
00:36:32,080 --> 00:36:34,040
So we have separate data.

524
00:36:34,040 --> 00:36:40,160
We can hold out test 
and validation data.

525
00:36:40,160 --> 00:36:45,280
And then we can train on the training data

526
00:36:45,280 --> 00:36:49,480
and then test on the testing data.

527
00:36:49,480 --> 00:36:51,960
But with graphs, you have this case

528
00:36:51,960 --> 00:36:55,400
where you can have 
this transductive kind of model,

529
00:36:55,400 --> 00:37:01,120
where you might have a case 
where you know the labels 
of some of the nodes,

530
00:37:01,120 --> 00:37:05,440
but you don't know some 
of the other nodes.

531
00:37:05,440 --> 00:37:11,760
But you can do this kind 
of like semi-supervised learning

532
00:37:11,760 --> 00:37:16,640
is that you can still train 
with this network

533
00:37:16,640 --> 00:37:18,920
and just your loss function

534
00:37:18,920 --> 00:37:22,040
is just incorporating 
your labeled nodes.

535
00:37:22,040 --> 00:37:26,000
And so at training time, 
you're not taking into account

536
00:37:26,000 --> 00:37:28,400
because we don't know the labels 
on these unknown nodes.

537
00:37:28,400 --> 00:37:31,920
But we can still proceed with training.

538
00:37:31,920 --> 00:37:34,480
But then at inference time,

539
00:37:34,480 --> 00:37:39,680
you could run inference 
on these unlabeled nodes.

540
00:37:39,680 --> 00:37:48,080
So that's the transductive model.

541
00:37:48,080 --> 00:37:51,920
So think about this example here

542
00:37:51,920 --> 00:37:57,920
where you want to do binary classification 
at the node level.

543
00:37:57,920 --> 00:38:03,840
So I just grabbed an example 
of some very large graph,

544
00:38:03,840 --> 00:38:05,600
but it could be millions of nodes.

545
00:38:05,600 --> 00:38:10,040
And these are partially labeled.

546
00:38:10,040 --> 00:38:18,000
So it's the same body as before 
that we use for the graph classification.

547
00:38:18,000 --> 00:38:21,680
And then we have vector outputs,

548
00:38:21,680 --> 00:38:27,080
which are the same dimension 
as the number of nodes.

549
00:38:27,080 --> 00:38:29,080
If we just remove the mean pooling,

550
00:38:29,080 --> 00:38:32,880
like in the node classification example,

551
00:38:32,880 --> 00:38:35,840
we get the vector output one by n.

552
00:38:35,840 --> 00:38:42,600
And then you can train 
using binary cross entropy loss

553
00:38:42,600 --> 00:38:52,080
on the nodes with the labels.

554
00:38:52,080 --> 00:38:57,360
So the problem is that, at least here, 
we're training,

555
00:38:57,360 --> 00:39:00,800
let's say you have a graph 
with millions of nodes,

556
00:39:00,800 --> 00:39:03,680
that for the training 
and that execution

557
00:39:03,680 --> 00:39:06,480
at every layer of the network,

558
00:39:06,480 --> 00:39:12,320
you have to read and process 
every one 
of the millions of nodes.

559
00:39:12,320 --> 00:39:16,200
And so you could very easily 
run into a situation

560
00:39:16,200 --> 00:39:20,200
where you just don't have 
the memory on your GPU

561
00:39:20,200 --> 00:39:24,520
or the resource that you're using

562
00:39:24,520 --> 00:39:29,080
to be able to store every node 
on every layer of your network.

563
00:39:29,080 --> 00:39:34,240
And then basically, 
we're not doing batches here,

564
00:39:34,240 --> 00:39:37,840
we can't do stochastic gradient descent.

565
00:39:37,840 --> 00:39:39,720
So we're not taking advantage 
of the fact

566
00:39:39,720 --> 00:39:44,200
that randomly picking some of these nodes

567
00:39:44,200 --> 00:39:46,120
and then running that.

568
00:39:46,120 --> 00:39:50,640
But there are ways to do it.

569
00:39:50,640 --> 00:39:55,520
And so three of the ways 
there's others as well

570
00:39:55,520 --> 00:40:00,520
is you can just randomly pick 
a subset of the nodes as a batch

571
00:40:00,520 --> 00:40:02,880
and run it

572
00:40:02,880 --> 00:40:07,040
and then just pick 
another subset 
of the nodes.

573
00:40:07,040 --> 00:40:11,520
And I'll explain what the issue 
with that is

574
00:40:11,520 --> 00:40:14,080
and then there's better ways,

575
00:40:14,080 --> 00:40:18,120
something called neighborhood sampling 
and graph partitioning,

576
00:40:18,120 --> 00:40:20,640
which is better.

577
00:40:20,640 --> 00:40:23,600
So the problem with just taking 
a random subset,

578
00:40:23,600 --> 00:40:24,680
so it's a small graph,

579
00:40:24,680 --> 00:40:32,960
and let's say we're just picking a node 
of one, size one, set one,

580
00:40:32,960 --> 00:40:38,640
that this node, because of that aggregation,

581
00:40:38,640 --> 00:40:41,720
it's dependent on the immediate neighbors

582
00:40:41,720 --> 00:40:45,000
of that same node in the previous layer.

583
00:40:45,000 --> 00:40:47,240
And in turn, each of these nodes

584
00:40:47,240 --> 00:40:47,960
are influenced

585
00:40:47,960 --> 00:40:52,040
by their immediate neighbors 
of the previous node.

586
00:40:52,040 --> 00:40:57,440
And so you can imagine 
with a deeper graph,

587
00:40:57,440 --> 00:41:03,080
and even if it's larger, 
with a deeper graph,

588
00:41:03,080 --> 00:41:10,720
that you could quickly start basically 
using up most of the network,

589
00:41:10,720 --> 00:41:14,520
so actually using most of the network.

590
00:41:14,520 --> 00:41:18,480
And just like in the convolutional 
neural nets,

591
00:41:18,480 --> 00:41:19,960
there's this notion 
of receptive field.

592
00:41:19,960 --> 00:41:24,600
So that growth, so the fact 
that from the original input,

593
00:41:24,600 --> 00:41:30,040
all of those nodes 
are effectively 
influencing this node,

594
00:41:30,040 --> 00:41:34,000
you can call that the receptive field 
for that particular node.

595
00:41:34,000 --> 00:41:44,880
So, yeah, and the problem is 
if you have fairly dense connections

596
00:41:44,880 --> 00:41:46,760
and you have lots of layers,

597
00:41:46,760 --> 00:41:51,440
you can quickly just expand 
to encompass every node in the graph.

598
00:41:51,440 --> 00:41:54,480
And so you haven't accomplished your goal

599
00:41:54,480 --> 00:41:58,360
of having these distinct batches.

600
00:41:58,360 --> 00:42:00,120
So the way neighborhood sampling works,

601
00:42:00,120 --> 00:42:04,600
you have the, I guess, 
a different graph here,

602
00:42:04,600 --> 00:42:06,800
but the same random sampling

603
00:42:06,800 --> 00:42:09,000
where you're just showing

604
00:42:09,000 --> 00:42:11,680
all of the nodes 
that influence this node here.

605
00:42:11,680 --> 00:42:15,240
With neighborhood sampling, 
you're doing the same thing,

606
00:42:15,240 --> 00:42:18,160
but you're creating just a max,

607
00:42:18,160 --> 00:42:19,320
a limit

608
00:42:19,320 --> 00:42:21,760
to how many nodes 
in your aggregation.

609
00:42:21,760 --> 00:42:24,360
So you're saying like in this case,

610
00:42:24,360 --> 00:42:27,520
I'm not going to use any more 
than three of the neighboring nodes,

611
00:42:27,520 --> 00:42:32,320
and you can just randomly 
select 
which three you use.

612
00:42:32,320 --> 00:42:35,080
But that case, at least you bound the growth

613
00:42:35,080 --> 00:42:37,320
as you go back into the network.

614
00:42:37,320 --> 00:42:45,120
Does that make sense? Okay.

615
00:42:45,120 --> 00:42:51,160
And there's a short notebook 
that goes 
into that a little bit more.

616
00:42:51,160 --> 00:42:55,560
And then graph partitioning takes a graph,

617
00:42:55,560 --> 00:42:58,920
and then what you do is you go 
and you clip some of the edges,

618
00:42:58,920 --> 00:43:04,520
and then you try to find these, 
you know, using some algorithms,

619
00:43:04,520 --> 00:43:10,920
you can find these kind of maximally 
connected subsets of the graph

620
00:43:10,920 --> 00:43:14,680
so that you partition them,

621
00:43:14,680 --> 00:43:16,080
and then you basically just remove them,

622
00:43:16,080 --> 00:43:19,400
and then you treat them as separate graphs.

623
00:43:19,400 --> 00:43:27,880
And you can do that partitioning 
on, 
you know,

624
00:43:27,880 --> 00:43:29,520
like every epoch,

625
00:43:29,520 --> 00:43:31,720
you can change the partitioning

626
00:43:31,720 --> 00:43:36,680
so that you cover some 
of these 
missing edges as well.

627
00:43:36,680 --> 00:43:37,640
Okay.

628
00:43:37,640 --> 00:43:48,040
Makes sense? Okay.

629
00:43:48,040 --> 00:43:50,480
I'm not going to go into detail here.

630
00:43:50,480 --> 00:43:54,040
It's in the book, but besides 
the mean pooling,

631
00:43:54,040 --> 00:43:57,560
there's a list in the book 
on different ways that you can,

632
00:43:57,560 --> 00:44:01,960
instead of just straight aggregation,

633
00:44:01,960 --> 00:44:08,400
you can do things like enhance 
the weight, the diagonal,

634
00:44:08,400 --> 00:44:11,080
a little bit more in the equation up there.

635
00:44:11,080 --> 00:44:12,320
You can add residual connections

636
00:44:12,320 --> 00:44:17,600
just like we saw in the lecture 
on ResNets.

637
00:44:17,600 --> 00:44:20,960
You can just add 
the previous hidden layer

638
00:44:20,960 --> 00:44:27,200
to the output 
of the activation function.

639
00:44:27,200 --> 00:44:30,840
For the aggregation function, 
you can weight them.

640
00:44:30,840 --> 00:44:35,440
So you're taking an average 
instead of a simple sum

641
00:44:35,440 --> 00:44:37,200
on the aggregation.

642
00:44:37,200 --> 00:44:39,880
This KIPF normalization is factoring in

643
00:44:39,880 --> 00:44:45,360
that if you have a node 
that's coming from, 
that has a lot of neighbors,

644
00:44:45,360 --> 00:44:51,720
maybe you don't want it 
to overly influence 
this aggregation here.

645
00:44:51,720 --> 00:44:57,200
And so you can normalize it 
a little bit there.

646
00:44:57,200 --> 00:45:00,240
And then instead of you can do 
a max pool aggregation.

647
00:45:00,240 --> 00:45:01,560
Instead of summing,

648
00:45:01,560 --> 00:45:05,480
you can just take 
the max 
of all the neighbors as well.

649
00:45:05,480 --> 00:45:09,400
So just like with regular neural networks,

650
00:45:09,400 --> 00:45:17,120
you can play with the different 
kinds 
of pooling operators.

651
00:45:17,120 --> 00:45:22,080
Okay.

652
00:45:22,080 --> 00:45:24,880
So then there is the analogy 
to transformers.

653
00:45:24,880 --> 00:45:28,480
You can also do aggregation by attention.

654
00:45:28,480 --> 00:45:35,320
And essentially you're creating 
a attention matrix,

655
00:45:35,320 --> 00:45:39,400
or learning an attention matrix,

656
00:45:39,400 --> 00:45:44,000
and then just kind of very similar 
to transformers

657
00:45:44,000 --> 00:45:46,840
is that so you're doing 
a linear transformation on the input.

658
00:45:46,840 --> 00:45:49,440
Remember when we took the input

659
00:45:49,440 --> 00:45:53,040
and we transformed it 
into the value vectors.

660
00:45:53,040 --> 00:45:55,280
And then we had the query and the key.

661
00:45:55,280 --> 00:45:58,960
There's not really a separate 
query and the key,

662
00:45:58,960 --> 00:46:03,600
but it is creating this attention matrix

663
00:46:03,600 --> 00:46:09,760
and then multiplying by this linear transformed value.

664
00:46:09,760 --> 00:46:12,760
So it is similar.

665
00:46:12,760 --> 00:46:18,720
And so if you draw like we did before,

666
00:46:18,720 --> 00:46:22,880
so this is the graph convolution 
that I was just showing you

667
00:46:22,880 --> 00:46:26,160
where you have the adjacency 
matrix 
which is fixed,

668
00:46:26,160 --> 00:46:28,280
and then you have the input

669
00:46:28,280 --> 00:46:31,360
which is going through a linear transformation,

670
00:46:31,360 --> 00:46:34,320
and then you're just multiplying 
by the adjacency matrix

671
00:46:34,320 --> 00:46:37,800
or that plus the identity.

672
00:46:37,800 --> 00:46:42,720
The graph attention, again, 
it's not exactly like the transformers,

673
00:46:42,720 --> 00:46:48,040
but it is now coming up 
with these attention weights

674
00:46:48,040 --> 00:46:49,480
and that's multiplying.

675
00:46:49,480 --> 00:46:54,480
So it's this kind of hyper network 
like I showed you for the transformers.

676
00:46:54,480 --> 00:46:57,360
It is learning some attention weights

677
00:46:57,360 --> 00:47:00,160
and then multiplying that

678
00:47:00,160 --> 00:47:01,600
by the linear transformed input.

679
00:47:01,600 --> 00:47:06,240
And just for comparison,

680
00:47:06,240 --> 00:47:12,400
I put the, oops, text got cut off here a little bit,

681
00:47:12,400 --> 00:47:17,080
but this was the regular dot product self-attention

682
00:47:17,080 --> 00:47:20,160
where you had the query and the keys

683
00:47:20,160 --> 00:47:21,760
and then you had the linear transform

684
00:47:21,760 --> 00:47:26,560
and then it was being multiplied 
by this attention matrix.

685
00:47:26,560 --> 00:47:29,760
So not quite like the transformer,

686
00:47:29,760 --> 00:47:41,480
but it does kind of integrate 
some notions of the self-attention.

687
00:47:41,480 --> 00:47:51,520
Okay? All right, we're actually 
on the last topic here.

688
00:47:51,520 --> 00:47:53,560
You guys are going to get out a bit early.

689
00:47:53,560 --> 00:47:57,480
So edge graphs, so this 
is actually quite simple.

690
00:47:57,480 --> 00:48:06,480
You can take, let's say, some graph 
that you have with the nodes.

691
00:48:06,480 --> 00:48:12,880
You can create these, you know, 
basically nodes corresponding to every edge.

692
00:48:12,880 --> 00:48:19,240
So these are just labeled 
by the nodes 
that they connect, right?

693
00:48:19,240 --> 00:48:21,600
So this is between nodes four and six,

694
00:48:21,600 --> 00:48:25,960
so we just call it like four slash six.

695
00:48:25,960 --> 00:48:30,000
So you can just add these at every edge

696
00:48:30,000 --> 00:48:33,800
and then just remove all of the nodes.

697
00:48:33,800 --> 00:48:39,080
And now you have, you know, 
an edge graph, actually,

698
00:48:39,080 --> 00:48:40,880
but this is representing, you know,

699
00:48:40,880 --> 00:48:44,880
assuming now that we have 
some useful information on the edges.

700
00:48:44,880 --> 00:48:48,040
So we have edge embedding vectors.

701
00:48:48,040 --> 00:48:51,480
We end up with what looked 
like the node graph,

702
00:48:51,480 --> 00:48:56,400
but it's just the edge 
embedding vectors.

703
00:48:56,400 --> 00:48:58,560
So everything that I just showed you before,

704
00:48:58,560 --> 00:49:02,320
now you can just apply and do, 
you know, whatever,

705
00:49:02,320 --> 00:49:11,080
classification and regression on the, 
with the node edge data.

706
00:49:11,080 --> 00:49:15,840
So very, very simple to handle.

707
00:49:15,840 --> 00:49:27,560
Okay? All right, it's a nice warm day, 
so we can go enjoy the weather.

708
00:49:27,560 --> 00:49:33,400
So next time my plan is to do 
kind of a RLHF spin

709
00:49:33,400 --> 00:49:36,080
on reinforcement learning.

710
00:49:36,080 --> 00:49:42,840
So I'm not going to try to recreate 
the other reinforcement class.

711
00:49:42,840 --> 00:49:47,640
And then my plan is to talk about

712
00:49:47,640 --> 00:49:51,960
this joint embedding predictive architecture 
for the last class,

713
00:49:51,960 --> 00:49:53,760
and then we'll have 
the project presentations.

714
00:49:53,760 --> 00:50:00,280
So like I said, I'll send out 
a Piazza note tonight

715
00:50:00,280 --> 00:50:02,400
with the assignment of who gets 
which slot

716
00:50:02,400 --> 00:50:04,040
on the 25th and the 30th.

717
00:50:04,040 --> 00:50:07,000
And then I'll also, well, it's Tuesday.

718
00:50:07,000 --> 00:50:08,120
We'll talk on Thursday,

719
00:50:08,120 --> 00:50:13,160
but it's a good chance I'll be in office 
for office hours this week as well.

720
00:50:13,160 --> 00:50:18,720
Okay, thanks.

721
00:50:18,720 --> 00:50:20,960
Thank you.

