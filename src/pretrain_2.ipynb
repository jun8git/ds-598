{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\DL_TRY\\src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"D:/DL_TRY/src\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\Anaconda3\\envs\\torch2.0.1_gpu_py39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to D:/DL_TRY/misc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from base.constants import *\n",
    "#from ..base.helpers import *\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, file_utils\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets \n",
    "train_texts = []\n",
    "with open(TRAIN_WITHOUT_LABELS_FILE, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        train_texts.append(line.strip())\n",
    "\n",
    "train_texts_with_labels = []\n",
    "with open(TRAIN_WITH_LABELS_FILE, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        train_texts_with_labels.append(line.strip())\n",
    "\n",
    "val_texts = []\n",
    "with open(VAL_WITHOUT_LABELS_FILE, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        val_texts.append(line.strip())\n",
    "\n",
    "val_texts_with_labels = []\n",
    "with open(VAL_WITH_LABELS_FILE, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        val_texts_with_labels.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = os.environ.get(\"TRANSFORMERS_CACHE\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\", \n",
    "                                              cache_dir=CACHE_DIR)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"You need to pick a pre-trained model from HuggingFace.\")\n",
    "    print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "Tokenized:  ['▁Thank', '▁you', '▁so', '▁much', ',', '▁Chris', '.', '▁And', '▁it', \"'\", 's', '▁truly', '▁a', '▁great', '▁honor', '▁to', '▁have', '▁the', '▁opportunity', '▁to', '▁come', '▁to', '▁this', '▁stage', '▁twice', ';', '▁I', \"'\", 'm', '▁extremely', '▁grateful', '.']\n",
      "Token IDs:  [25689, 398, 221, 5045, 4, 31745, 5, 3493, 442, 25, 7, 87607, 10, 6782, 20338, 47, 765, 70, 54591, 47, 1380, 47, 903, 36541, 186351, 74, 87, 25, 39, 111531, 225876, 5]\n",
      "CLS token: <s> 0\n",
      "SEP token: </s> 2\n"
     ]
    }
   ],
   "source": [
    "# Have a knowledge of the tokenizer.\n",
    "# Print the original sentence.\n",
    "print(' Original: ', train_texts[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(train_texts[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_texts[0])))\n",
    "\n",
    "# Print special marks and their IDs.\n",
    "print(\"CLS token:\", tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(\"SEP token:\", tokenizer.sep_token, tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to convert texts with <eol> and <eob> tags to output format\n",
    "def mark_breakpoints(input_ids):\n",
    "    # Initialize a list of zeros for marks, same length as input_ids\n",
    "    marks = [0] * len(input_ids)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(input_ids):\n",
    "        # tokenizer(\"\") == {'input_ids': [0, 2], 'attention_mask': [1, 1]}\n",
    "        # tokenizer(\"<eob>\") == {'input_ids': [0, 4426, 13, 3522, 2740, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
    "        # tokenizer(\"<eol>\") == {'input_ids': [0, 4426, 13, 929, 2740, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
    "        # Check for the patterns corresponding to <eob> or <eol>\n",
    "        if input_ids[i:i+4] == [4426, 13, 3522, 2740] or input_ids[i:i+4] == [4426, 13, 929, 2740]:\n",
    "            # Determine the marker value, 2 for <eob> and 1 for <eol>\n",
    "            mark_value = 2 if input_ids[i+2] == 3522 else 1\n",
    "            if i > 0:  # Ensure it is not at the start of the sequence\n",
    "                marks[i-1] = mark_value  # Mark the previous token\n",
    "            # Remove the tokens associated with <eob> or <eol>\n",
    "            del marks[i:i+4]\n",
    "            del input_ids[i:i+4]\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return marks\n",
    "\n",
    "def texts2labels(texts_with_labels):\n",
    "    labels = []\n",
    "    for i in texts_with_labels:\n",
    "        label = mark_breakpoints(tokenizer(i)['input_ids'])\n",
    "        labels.append(label)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = texts2labels(train_texts_with_labels)\n",
    "val_labels = texts2labels(val_texts_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  300\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_texts:\n",
    "    input_ids = tokenizer(sent)['input_ids']\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_outputs(texts, labels, max_length=330, print_samples = False):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    padded_labels = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in texts:\n",
    "        encoded_dict = tokenizer(\n",
    "                            sent,                          # Sentence to encode.\n",
    "                            max_length = 330,              # Pad & truncate all sentences.\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask = True,  # Construct attn. masks.\n",
    "                            return_tensors = 'pt',         # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Pad labels\n",
    "    padded_labels = torch.tensor([label + [-1] * (max_length - len(label)) for label in labels])\n",
    "\n",
    "    # print samples\n",
    "    if print_samples == True:\n",
    "        print('Original: ', texts[0])\n",
    "        print('Token IDs:', input_ids[0])\n",
    "        print('Attention Masks:', attention_masks[0])\n",
    "        print('labels', padded_labels[0])\n",
    "\n",
    "    return input_ids, attention_masks, padded_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs_outputs(texts, labels, max_length=330, print_samples = False):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    padded_labels = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in texts:\n",
    "        encoded_dict = tokenizer(\n",
    "                            sent,                          # Sentence to encode.\n",
    "                            max_length = 330,              # Pad & truncate all sentences.\n",
    "                            padding='max_length',\n",
    "                            return_attention_mask = True,  # Construct attn. masks.\n",
    "                            return_tensors = 'pt',         # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    #input_ids = torch.cat(input_ids, dim=0)\n",
    "    #attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    # Pad labels\n",
    "    padded_labels = torch.tensor([label + [0] * (max_length - len(label)) for label in labels])\n",
    "\n",
    "    # print samples\n",
    "    if print_samples == True:\n",
    "        print('Original: ', texts[0])\n",
    "        print('Token IDs:', input_ids[0])\n",
    "        print('Attention Masks:', attention_masks[0])\n",
    "        print('labels', padded_labels[0])\n",
    "\n",
    "    return input_ids, attention_masks, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
      "Token IDs: tensor([[     0,  25689,    398,    221,   5045,      4,  31745,      5,   3493,\n",
      "            442,     25,      7,  87607,     10,   6782,  20338,     47,    765,\n",
      "             70,  54591,     47,   1380,     47,    903,  36541, 186351,     74,\n",
      "             87,     25,     39, 111531, 225876,      5,      2,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1]])\n",
      "Attention Masks: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "labels tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks, train_padded_labels = prepare_inputs_outputs(train_texts, train_labels, 330, True)\n",
    "val_input_ids, val_attention_masks, val_padded_labels = prepare_inputs_outputs(val_texts, val_labels, 330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubtitleDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, padded_labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.padded_labels = padded_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)       \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" input_id = torch.tensor(self.input_ids[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_masks[idx], dtype=torch.long)\n",
    "        padded_label = torch.tensor(self.padded_labels[idx], dtype=torch.long) \"\"\"\n",
    "        \n",
    "        # 返回处理后的单个样本\n",
    "        #return input_id, attention_mask, padded_label\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.padded_labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220,068 training samples\n",
      "27,509 validation samples\n",
      "Sample input size:  torch.Size([32, 1, 330])\n",
      "Sample input: \n",
      " tensor([[[    0, 25689,   398,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0,    87,   765,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0,  3493,    87,  ...,     1,     1,     1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    0,  4966,  2367,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0,    15,  2729,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0,    15,  2729,  ...,     1,     1,     1]]])\n",
      "Sample mask: \n",
      " tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]])\n",
      "Sample output: \n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SubtitleDataset(train_input_ids, train_attention_masks, train_padded_labels)\n",
    "val_dataset = SubtitleDataset(val_input_ids, val_attention_masks, val_padded_labels)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(train_dataset)))\n",
    "print('{:>5,} validation samples'.format(len(val_dataset)))\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_mask, sample_y = next(dataiter)\n",
    "\n",
    "print('Sample input size: ', sample_x.size())# batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print('Sample mask: \\n', sample_mask)\n",
    "print('Sample output: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\")\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 3)\n",
    "model.num_labels = 3\n",
    "\n",
    "model.to(device)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=2e-5, \n",
    "                              betas=(0.9, 0.999),\n",
    "                              eps = 1e-8, \n",
    "                              weight_decay=0.0005,\n",
    "                              )\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 加载分词器和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\")\n",
    "model.to(device)\n",
    "#model.classifier = torch.nn.Linear(model.classifier.in_features, 2)\n",
    "#model.num_labels = 3\n",
    "#model.output_attentions = False\n",
    "mask = sample_mask[0]\n",
    "\n",
    "# 准备输入文本\n",
    "text = train_texts[2]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")  # 确保返回 PyTorch 张量\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():  # 确保不计算梯度\n",
    "    outputs = model(sample_x[0].to(device),sample_mask[0].to(device))\n",
    "\n",
    "# 计算概率\n",
    "probabilities = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 获取每个标记的最大概率标签\n",
    "predictions = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "# 输出结果\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # 确保不计算梯度\n",
    "    outputs = model(sample_x[0].to(device), sample_mask[0].to(device))#, sample_y[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "accumulation_steps = 2\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss / accumulation_steps  # Scale loss\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            # Only perform optimization step every 'accumulation_steps'\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()  # Zero the gradients after optimization\n",
    "\n",
    "        total_train_loss += loss.item() * accumulation_steps  # Unscale the loss\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_loader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "\n",
    "            result = model(b_input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loger, train_dataloader, model, optimizer, device, processor):\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in progress_bar:\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "        # debug\n",
    "        # print(\"Pixel_values\",pixel_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, pixel_values=pixel_values, labels=input_ids\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with loss info\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loger, train_dataloader, model, optimizer, device, processor):\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in progress_bar:\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "        # debug\n",
    "        # print(\"Pixel_values\",pixel_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, pixel_values=pixel_values, labels=input_ids\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar with loss info\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    logger, epoch, save_path, best_score, val_dataloader,model, processor, device\n",
    "):\n",
    "    # model_evaluate = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\", cache_dir=f\"{DEMO_SAVE_PATH}/model_{epoch}\")\n",
    "    # model_evaluate.to(device)\n",
    "    model.eval()\n",
    "    caption_val = []\n",
    "    plot_captions_dict = {}\n",
    "    for idx, batch in enumerate(val_dataloader):\n",
    "        image_ids = batch.pop(\"image_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "        # debug\n",
    "        # print(\"pixel_values\",pixel_values)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "            #outputs = model.generate(pixel_values=pixel_values)\n",
    "            # debug\n",
    "            #print(\"Raw Output:\", outputs)\n",
    "\n",
    "        # Decode the generated ids to text\n",
    "        generated_captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        # debug\n",
    "        #print(\"Decoded Output:\", generated_captions)\n",
    "\n",
    "        # Store the generated captions\n",
    "        for img_id, caption in zip(image_ids, generated_captions):\n",
    "            caption_val.append(\n",
    "                {\"image_id\": img_id.item(), \"caption\": caption}\n",
    "            )  # Used for VizWizEvalCap\n",
    "            plot_captions_dict[img_id.item()] = caption  # Used for plotting\n",
    "\n",
    "    # Save the generated captions to a json file\n",
    "    # Change the path\n",
    "    with open(f\"{save_path}/generated_captions_{i}.json\", \"w\") as f:\n",
    "        json.dump(caption_val, f, indent=4)\n",
    "\n",
    "    # Change the path\n",
    "    vizwizRes = val_dataset.dataset.vizwiz.loadRes(\n",
    "        f\"{save_path}/generated_captions_{i}.json\"\n",
    "    )\n",
    "    vizwizEval = VizWizEvalCap(val_dataset.dataset.vizwiz, vizwizRes)\n",
    "    vizwizEval.evaluate()\n",
    "\n",
    "    logger.info(f\"Validation scores at epoch: {epoch}\")\n",
    "    for method in vizwizEval.eval:\n",
    "        logger.info(f\"  Method: {method}, Score: {vizwizEval.eval[method]:.4f}\")\n",
    "\n",
    "    return vizwizEval, vizwizRes, plot_captions_dict, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "         0,  2,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  2,  0, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,\n",
       "         2,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  2,  0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  3493,    87,  5154,   450, 43699, 21286,     4,  2831,   538,\n",
       "          6637,    15,  9083,  2594, 19185,    16,    87,  3871,   450,     5,\n",
       "            15,  2729, 46526,    56,    16,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  3493,    87,  5154,   450, 43699, 21286,     4,  2831,   538,\n",
       "          6637,    15,  9083,  2594, 19185,    16,    87,  3871,   450,     5,\n",
       "            15,  2729, 46526,    56,    16,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_texts[2], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 3.7575, -2.8115],\n",
       "         [ 5.2627, -5.0432],\n",
       "         [ 4.3903, -4.2201],\n",
       "         [ 5.7408, -5.5647],\n",
       "         [ 4.3519, -3.8157],\n",
       "         [ 4.3743, -4.7460],\n",
       "         [ 3.1060, -2.8904],\n",
       "         [-1.5279,  1.0472],\n",
       "         [ 4.3476, -4.1248],\n",
       "         [ 6.3177, -6.1020],\n",
       "         [ 5.1890, -4.9129],\n",
       "         [ 5.4288, -5.2133],\n",
       "         [ 5.7272, -5.2702],\n",
       "         [ 4.7480, -4.4442],\n",
       "         [ 5.4618, -5.0336],\n",
       "         [ 4.3085, -4.0404],\n",
       "         [ 5.3212, -5.7122],\n",
       "         [ 5.9274, -5.9662],\n",
       "         [ 4.1552, -4.1624],\n",
       "         [ 5.0201, -4.9719],\n",
       "         [ 5.4669, -5.7374],\n",
       "         [ 6.0409, -5.7752],\n",
       "         [ 5.7806, -6.0583],\n",
       "         [ 5.4140, -5.2766],\n",
       "         [ 3.1619, -2.6765],\n",
       "         [ 3.5358, -2.9870],\n",
       "         [ 0.8151, -0.7760],\n",
       "         [ 5.5859, -5.7451],\n",
       "         [ 5.3556, -5.2577],\n",
       "         [ 5.2833, -5.1451],\n",
       "         [ 5.4452, -5.0072],\n",
       "         [ 3.7298, -3.4053],\n",
       "         [-2.2294,  1.6910],\n",
       "         [ 4.1419, -3.1060],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015],\n",
       "         [ 4.1383, -3.1015]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.9860e-01, 1.4013e-03],\n",
       "         [9.9997e-01, 3.3434e-05],\n",
       "         [9.9982e-01, 1.8216e-04],\n",
       "         [9.9999e-01, 1.2305e-05],\n",
       "         [9.9972e-01, 2.8362e-04],\n",
       "         [9.9989e-01, 1.0940e-04],\n",
       "         [9.9752e-01, 2.4814e-03],\n",
       "         [7.0762e-02, 9.2924e-01],\n",
       "         [9.9979e-01, 2.0911e-04],\n",
       "         [1.0000e+00, 4.0384e-06],\n",
       "         [9.9996e-01, 4.0998e-05],\n",
       "         [9.9998e-01, 2.3889e-05],\n",
       "         [9.9998e-01, 1.6745e-05],\n",
       "         [9.9990e-01, 1.0182e-04],\n",
       "         [9.9997e-01, 2.7663e-05],\n",
       "         [9.9976e-01, 2.3661e-04],\n",
       "         [9.9998e-01, 1.6153e-05],\n",
       "         [9.9999e-01, 6.8339e-06],\n",
       "         [9.9976e-01, 2.4412e-04],\n",
       "         [9.9995e-01, 4.5759e-05],\n",
       "         [9.9999e-01, 1.3615e-05],\n",
       "         [9.9999e-01, 7.3850e-06],\n",
       "         [9.9999e-01, 7.2181e-06],\n",
       "         [9.9998e-01, 2.2758e-05],\n",
       "         [9.9710e-01, 2.9049e-03],\n",
       "         [9.9853e-01, 1.4673e-03],\n",
       "         [8.3077e-01, 1.6923e-01],\n",
       "         [9.9999e-01, 1.1995e-05],\n",
       "         [9.9998e-01, 2.4587e-05],\n",
       "         [9.9997e-01, 2.9580e-05],\n",
       "         [9.9997e-01, 2.8878e-05],\n",
       "         [9.9920e-01, 7.9600e-04],\n",
       "         [1.9447e-02, 9.8055e-01],\n",
       "         [9.9929e-01, 7.1117e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04],\n",
       "         [9.9928e-01, 7.1701e-04]]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.11G/1.11G [01:45<00:00, 10.5MB/s]\n",
      "d:\\program\\Anaconda3\\envs\\torch2.0.1_gpu_py39\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\15292\\.cache\\huggingface\\hub\\models--igorsterner--xlmr-multilingual-sentence-segmentation. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer_config.json: 100%|██████████| 1.18k/1.18k [00:00<00:00, 119kB/s]\n",
      "tokenizer.json: 100%|██████████| 17.1M/17.1M [00:01<00:00, 15.7MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 280/280 [00:00<00:00, 21.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = os.environ.get(\"TRANSFORMERS_CACHE\")\n",
    "pipe = pipeline(\"token-classification\", model=\"igorsterner/xlmr-multilingual-sentence-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': '|',\n",
       "  'score': 0.9292383,\n",
       "  'index': 7,\n",
       "  'word': '.',\n",
       "  'start': 24,\n",
       "  'end': 25},\n",
       " {'entity': '|',\n",
       "  'score': 0.9805526,\n",
       "  'index': 32,\n",
       "  'word': '.',\n",
       "  'start': 130,\n",
       "  'end': 131}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(train_texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.0.1_gpu_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
