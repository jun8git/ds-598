{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "E7exkyrqPru0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "#from base.constants import *\n",
        "#from ..base.helpers import *\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline, file_utils\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPVW1sswPru0",
        "outputId": "4c7c8e55-9ccb-42f1-9f68-33addc868ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyNS9hycPxms",
        "outputId": "9edf330c-7895-4409-f3ea-6f6b8006776a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "q28SIhfsP9sE"
      },
      "outputs": [],
      "source": [
        "DIFFUSION_MODEL_VTT = '/content/drive/My Drive/data/dl4ds_diffusion_models.vtt'\n",
        "GRAPH_NN_VTT = '/content/drive/My Drive/data/dl4ds_graph_nn.vtt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "x0HCNSByPru0"
      },
      "outputs": [],
      "source": [
        "# Define the regex pattern for timestamps\n",
        "time_pattern = re.compile(\n",
        "    r'(\\d{2}:\\d{2}\\.\\d{3} *--> *\\d{2}:\\d{2}\\.\\d{3})|'\n",
        "    r'(\\d{2}:\\d{2}:\\d{2}\\.\\d{3} *--> *\\d{2}:\\d{2}:\\d{2}\\.\\d{3})'\n",
        ")\n",
        "\n",
        "# Function to process lines from a file and split by period, skipping the first line\n",
        "def process_and_split_file(file_path):\n",
        "    all_text = ''\n",
        "    first_line_skipped = False\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if not first_line_skipped:\n",
        "                first_line_skipped = True\n",
        "                continue\n",
        "            stripped_line = line.strip()\n",
        "            # Skip empty lines or timestamps\n",
        "            if not stripped_line or time_pattern.match(stripped_line):\n",
        "                continue\n",
        "            # Add a space before appending if the accumulated text is not empty\n",
        "            if all_text and not all_text.endswith(' '):\n",
        "                all_text += ' '\n",
        "            all_text += stripped_line\n",
        "\n",
        "    # Split the text at each period, ensuring each segment ends with a period\n",
        "    split_texts = [text.strip() + '.' for text in all_text.split('.') if text.strip()]\n",
        "    return split_texts\n",
        "\n",
        "# Assume DIFFUSION_MODEL_VTT and GRAPH_NN_VTT are defined\n",
        "diff_texts = process_and_split_file(DIFFUSION_MODEL_VTT)\n",
        "graph_texts = process_and_split_file(GRAPH_NN_VTT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bYXvwebmPru0"
      },
      "outputs": [],
      "source": [
        "CACHE_DIR = os.environ.get(\"TRANSFORMERS_CACHE\")\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\",\n",
        "                                              cache_dir=CACHE_DIR)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"You need to pick a pre-trained model from HuggingFace.\")\n",
        "    print(\"Exception: \", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hQXWVmNPPru0"
      },
      "outputs": [],
      "source": [
        "def prepare_inputs(texts, max_length=330, print_samples = False):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in texts:\n",
        "        encoded_dict = tokenizer(\n",
        "                            sent,                          # Sentence to encode.\n",
        "                            max_length = max_length,              # Pad & truncate all sentences.\n",
        "                            padding='max_length',\n",
        "                            return_attention_mask = True,  # Construct attn. masks.\n",
        "                            return_tensors = 'pt',         # Return pytorch tensors.\n",
        "                    )\n",
        "\n",
        "        # Add the encoded sentence to the list.\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    # print samples\n",
        "    if print_samples == True:\n",
        "        print('Original: ', texts[0])\n",
        "        print('Token IDs:', input_ids[0])\n",
        "        print('Attention Masks:', attention_masks[0])\n",
        "\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dgd5zS7jPru1"
      },
      "outputs": [],
      "source": [
        "train_input_ids_1, train_attention_masks_1 = prepare_inputs(diff_texts)\n",
        "train_input_ids_2, train_attention_masks_2 = prepare_inputs(graph_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "20ZWxSnlPru1"
      },
      "outputs": [],
      "source": [
        "class SubtitleDataset(Dataset):\n",
        "    def __init__(self, input_ids, attention_masks):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_masks = attention_masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUxvbzB-Pru1",
        "outputId": "c15ec987-58e7-46f8-d92a-e911e763edbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  395 DIFFUSION_MODEL_VTT samples\n",
            "  285 GRAPH_NN_VTT samples\n"
          ]
        }
      ],
      "source": [
        "diff_dataset = SubtitleDataset(train_input_ids_1, train_attention_masks_1)\n",
        "graph_dataset = SubtitleDataset(train_input_ids_2, train_attention_masks_2)\n",
        "\n",
        "#diff_dataset = Subset(diff_dataset, range(1000))\n",
        "#graph_dataset = Subset(graph_dataset, range(50))\n",
        "\n",
        "print('{:>5,} DIFFUSION_MODEL_VTT samples'.format(len(diff_dataset)))\n",
        "print('{:>5,} GRAPH_NN_VTT samples'.format(len(graph_dataset)))\n",
        "\n",
        "batch_size = 16\n",
        "diff_loader = DataLoader(diff_dataset, batch_size, shuffle=False)\n",
        "graph_loader = DataLoader(graph_dataset, batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_cdoaWlPru1",
        "outputId": "81761998-55f5-4b18-cc79-4a25b3a58bf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XLMRobertaForTokenClassification(\n",
              "  (roberta): XLMRobertaModel(\n",
              "    (embeddings): XLMRobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): XLMRobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x XLMRobertaLayer(\n",
              "          (attention): XLMRobertaAttention(\n",
              "            (self): XLMRobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): XLMRobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): XLMRobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): XLMRobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the best model\n",
        "best_model = AutoModelForTokenClassification.from_pretrained(\"igorsterner/xlmr-multilingual-sentence-segmentation\")\n",
        "best_model.classifier = torch.nn.Linear(best_model.classifier.in_features, 3)\n",
        "best_model.num_labels = 3\n",
        "\n",
        "path = '/content/drive/My Drive/best_model/state_dict.pt'\n",
        "best_model.load_state_dict(torch.load(path))\n",
        "best_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ma29bFbuPru1"
      },
      "outputs": [],
      "source": [
        "def remove_special_tags(text):\n",
        "    # Remove start and end tags\n",
        "    text = text.replace(\"<s>\", \"\")\n",
        "    text = text.replace(\"</s>\", \"\")\n",
        "    return text\n",
        "\n",
        "best_model.eval()\n",
        "\n",
        "def add_eol_eob(loader):\n",
        "    final_predictions = []\n",
        "    modified_input_ids = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in loader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            result = best_model(b_input_ids,\n",
        "                                attention_mask=b_input_mask,\n",
        "                                return_dict=True)\n",
        "\n",
        "            logits = result.logits\n",
        "            masked_logits = logits[b_input_mask.bool()].view(-1, logits.size(-1))\n",
        "            probabilities = torch.softmax(masked_logits, dim=-1)\n",
        "            predictions = torch.argmax(probabilities, dim=-1)\n",
        "\n",
        "            b_input_ids = b_input_ids[b_input_mask.bool()]\n",
        "\n",
        "            final_predictions.append(predictions)\n",
        "            modified_input_ids.append(b_input_ids)\n",
        "\n",
        "    # Flatten the list of tensors into a single tensor\n",
        "    final_predictions = torch.cat(final_predictions)\n",
        "    modified_input_ids = torch.cat(modified_input_ids)\n",
        "\n",
        "    # Include special tokens based on predictions before converting to tokens\n",
        "    full_input_ids = []\n",
        "    for id, pred in zip(modified_input_ids, final_predictions):\n",
        "        full_input_ids.append(id)\n",
        "        if pred == 1:\n",
        "            # Append <eol> special token ids\n",
        "            eol_ids = tokenizer(\"<eol>\")['input_ids'][1:-1]  # exclude [CLS] and [SEP]\n",
        "            full_input_ids.extend(eol_ids)\n",
        "        elif pred == 2:\n",
        "            # Append <eob> special token ids\n",
        "            eob_ids = tokenizer(\"<eob>\")['input_ids'][1:-1]  # exclude [CLS] and [SEP]\n",
        "            full_input_ids.extend(eob_ids)\n",
        "\n",
        "    # Convert the full list of input_ids to tokens\n",
        "    full_text = tokenizer.decode(full_input_ids)\n",
        "\n",
        "    clean_text = remove_special_tags(full_text)\n",
        "    clean_text.strip()\n",
        "\n",
        "    return clean_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IhvYRTfRPru1"
      },
      "outputs": [],
      "source": [
        "diff_labeled = add_eol_eob(diff_loader)\n",
        "graph_labeled = add_eol_eob(graph_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LQtbD3FZPru1"
      },
      "outputs": [],
      "source": [
        "DIFFUSION_MODEL_TEXT = '/content/drive/My Drive/data/dl4ds_diffusion_models.txt'\n",
        "GRAPH_NN_TEXT = '/content/drive/My Drive/data/dl4ds_graph_nn.txt'\n",
        "\n",
        "def write_text_to_file(text, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "\n",
        "write_text_to_file(diff_labeled, DIFFUSION_MODEL_TEXT)\n",
        "write_text_to_file(graph_labeled, GRAPH_NN_TEXT)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
